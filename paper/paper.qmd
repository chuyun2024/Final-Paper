---
title: "2025 US School Shooting Predictions and Solutions"
subtitle: "Leveraging Forecasting to Shape Policy and Protect Communities"
author: 
  - Yun Chu
thanks: "Code and data are available at: https://github.com/chuyun2024/School-Shooting-Analysis."
date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(ggplot2)
library(dplyr)
library(maps)
library(knitr)
library(randomForest)
library(here)
#### Read data ####
data <- read_csv("../data/02-analysis_data/analysis_data.csv")
```


# Introduction

Overview paragraph

Estimand paragraph

Results paragraph

Why it matters paragraph

Telegraphing paragraph: The remainder of this paper is structured as follows. @sec-data....



Paragraph of Estimand!


# Data {#sec-data}

## Overview

The dataset has 416 entries, with each entry representing a unique school shooting incident. Incidents occurring during after-hours events, accidental gun discharges that only injured the individual handling the firearm, and private suicides that did not endanger other children were excluded from consideration. Additionally, shootings at colleges and universities, which involve young adults rather than children, were not included in the analysis [@washingtonpost_school_shootings]. These entries cover 50 variables that provide information about the schools, locations, date and time of shooting, shooters details, number of killed and injured, and the relationship of the shooter to school, the weapon type and source.

As the federal government does not consistently track school shootings, this dataset from *The Washington Post* fills a critical gap. It was carefully assembled using information from diverse sources, including news articles, open-source databases, law enforcement reports, and direct inquiries to schools and police departments. Although sources like FBI crime reports and local school incident logs were reviewed, they lack the detail and comprehensive coverage of this dataset. Its unparalleled breadth and depth make it the strongest foundation for predictive modeling and generating actionable insights.

We use the statistical programming language R [@citeR] to download, clean, analyze and model the US School Shooting Data. The US School Shooting dataset is downloaded from The Washington Post [@washingtonpost_school_shootings_data] . The following libraries are utilized in this paper:

- tidyverse [@tidyverse2024]
- dplyr [@dplyr2024]
- lubridate [@lubridate2024]
- readr [@readr2024]
- stringr [@stringr2024]
- arrow [@arrow2024]
- testthat [@testthat2024]
- randomForest [@randomForest2024]
- ggplot2
- maps
- knitr

## Measurement

The dataset, compiled by The Washington Post, translates real-world school shooting incidents into structured entries by aggregating information from news articles, open-source databases, law enforcement reports, and direct calls to schools. Only verified incidents, such as shootings during school hours or on school property, were included. Events like after-hours shootings, private suicides, or accidental discharges without other injuries were excluded [@washingtonpost_school_shootings].

## Summary Statistics & Relationship Between Variables

In this dataset, there are three variables that have relationship: $ Causalities = Killed + Injured $.

@tbl-casualties_summary_statistics summarizes the mean, median and standard deviation of casualties from school shooting events in US from 1999 to today. The standard deviation of 3.72 indicates that while most incidents have casualties close to the mean, there is a wide range of variability, with some incidents having significantly higher number of casualties.

```{r}
#| label: tbl-casualties_summary_statistics
#| fig-cap: "Summary Statistics for Casualties"
#| echo: false

# Ensure the data contains the 'casualties' column
# Calculate the mean, median, and standard deviation
casualties_summary <- data %>%
  summarise(
    Mean = mean(casualties, na.rm = TRUE),
    Median = median(casualties, na.rm = TRUE),
    `Standard Deviation` = sd(casualties, na.rm = TRUE)
  )

# Display the summary in a beautiful table
kable(casualties_summary, caption = "Summary Statistics for Casualties") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)
```


## Outcome variables

The outcome variable for this analysis is number of causalities for each state in 2025.


@fig-Shooting_Casaulties_by_State visualizes the number of school shooting incidents in US by state for all data since 1999. California, Texas, Florida and North Carolina all have more than 20 school shooting incidents in the past 25 years while other states has less than 20 school shootings.

```{r}
#| label: fig-Shooting_Casaulties_by_State
#| fig-cap: "US School Shooting Casaulties by State"
#| echo: false

# Aggregate data to calculate total casualties by state
state_counts <- data %>%
  group_by(state) %>%
  summarise(total_casualties = sum(casualties, na.rm = TRUE))

# Get US map data
us_map <- map_data("state")

# Ensure state names in state_counts are lowercase to match map_data
state_counts <- state_counts %>%
  mutate(state = tolower(state))

# Merge map data with shooting casualties data
map_data <- us_map %>%
  left_join(state_counts, by = c("region" = "state"))

# Replace missing values in `total_casualties` with 0 for states with no data
map_data$total_casualties[is.na(map_data$total_casualties)] <- 0

# Plot the map
ggplot(map_data, aes(long, lat, group = group, fill = total_casualties)) +
  geom_polygon(color = "white") +
  scale_fill_gradient(
    low = "#E6F7FF",  # Light blue
    high = "#0072B2",  # Dark blue
    name = "Total Casualties",
    breaks = c(0, 50, 100, 150, 200)  # Adjust breaks based on your data
  ) +
  labs(
    title = "US School Shooting Casualties by State",
    subtitle = "Number of casualties visualized geographically",
    x = NULL,
    y = NULL
  ) +
  coord_fixed(1.3) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = ggplot2::element_text(hjust = 0.5, face = "bold", size = 13, margin = ggplot2::margin(t = 10, b = 10)),
    plot.subtitle = ggplot2::element_text(hjust = 0.5, size = 10, margin = ggplot2::margin(t = 0, b = 20)),
    legend.position = "right",  # Alternative: Use "bottom" for a horizontal legend
    legend.key.height = unit(1, "cm"),  # Adjust legend size
    axis.text = ggplot2::element_blank(),
    axis.ticks = ggplot2::element_blank(),
    panel.grid = ggplot2::element_blank(),
    plot.margin = ggplot2::margin(t = 20, r = 20, b = 20, l = 20)  # Increase margins for PDF export
  )


```

## Predictor variables

Predictor variables in this analysis is state and year. 

 - State represents the state where the schools are located.
 
 - Year represents the shooting year.

@fig-Number_of_Incidents_Over_Time shows the number of school shooting incidents in US since 1999. The number of school shooting incidents almost doubled since 2020 compared with previous years.


```{r}
#| label: fig-Number_of_Incidents_Over_Time
#| fig-cap: "Number of US School Shooting Incidents Over Time"
#| echo: false
#| eval: false

ggplot(data, aes(x = date)) +
  geom_histogram(binwidth = 60, fill = '#3FA0FF', color = 'white', alpha = 0.8) +
  labs(
    title = "Number of US School Shooting Incidents Over Time",
    subtitle = "Visualizing school shooting trends in the US",
    x = "Date",
    y = "Incident Count"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 13, margin = margin(b = 15)),
    plot.subtitle = element_text(hjust = 0.5, size = 10, margin = margin(b = 10)),
    axis.title.x = element_text(margin = margin(t = 10)),
    axis.title.y = element_text(margin = margin(r = 10)),
    panel.grid.major = element_line(color = "#E5E5E5"),
    panel.grid.minor = element_blank()
  )

```



# Model

## Model Set-Up

To model the total number of casualties resulting from school shooting incidents, a **Random Forest regression model** is employed. The model is designed to predict casualties based on state-level and temporal features derived from the dataset. The Random Forest algorithm is chosen for its robustness to non-linear relationships, its ability to handle interactions among predictors, and its ability to integrate categorical variables without creating dummy variables.

The mathematical representation of the model is: $C = f(X_1, X_2, ..., X_p) + \epsilon$

where:

- $C$: The total number of casualties (sum of fatalities and injuries) for a given year and state.  
- $f(\cdot)$: The function estimated by the Random Forest model, representing the aggregate prediction from all decision trees.  
- $X_1, X_2, ..., X_p$: Predictors, including:  
  - $X_1$: Year (numeric), capturing temporal trends.  
  - $X_2$: State (categorical), capturing regional effects.  
- $\epsilon$: The error term, representing unobserved factors affecting the number of casualties.  

## Model Justification
- **Year** is included to account for temporal trends in school shootings, reflecting potential increases or decreases over time.
- **State** is treated as a categorical variable to capture location-specific effects, such as differences in legislation, policing, or socioeconomic factors.

The choice of these features aligns with the data section, ensuring that variables with potential predictive value are incorporated without overfitting or introducing unnecessary complexity.

### Assumptions and Limitations
- **Assumptions**:
  1. Casualties are conditionally independent given the predictors.
  2. The relationship between predictors and casualties can be approximated by the ensemble of decision trees.
  3. Data is representative of underlying trends and free of major sampling biases.

- **Limitations**:
  1. The model may not generalize well to future data if underlying trends shift dramatically (e.g., policy changes or societal events).
  2. Random Forests lack explicit interpretability compared to simpler models.


### Software Implementation
The model is implemented in **R**, using the randomForest [@randomforest] package for training and evaluation.

### Model Validation
- **Train/Test Splits**:
  The data is split into training (80%) and test (20%) sets. The model is trained on the former and validated on the latter to ensure generalizability.

- **Evaluation Metrics**:
  - **Root Mean Squared Error (RMSE)** and **Mean Absolute Error (MAE)** are calculated to assess prediction accuracy.
  - Final metrics:
    - RMSE: 1.491141 casualties
    - MAE: 1.096164 casualties

### Alternative Models Considered
- **Linear Regression**:
  - Strengths: Simplicity and interpretability.
  - Weaknesses: Inadequate for capturing non-linear relationships and interactions present in the data.

- **Gradient Boosting Machines (GBM)**:
  - Strengths: Often more accurate due to its iterative learning.
  - Weaknesses: Higher risk of overfitting, more computationally expensive and less intepretibility.

The Random Forest model was chosen over these alternatives for its balance of flexibility, robustness, and interpretability of feature importance.



# Results

Our results are summarized in @tbl-modelresults.


```{r}
#| echo: false
#| eval: true
#| label: fig-predicted_shooting_visualization
#| tbl-cap: "Predicted 2025 US School Shootings Numbers"
#| warning: false

#### Step 1: Load the Data and Model ####
# Load cleaned data
clean_data <- read.csv("../data/02-analysis_data/no_outlier_data.csv")

# Ensure 'state' is a factor
clean_data$state <- as.factor(clean_data$state)

# Load the Random Forest model
rf_model_clean <- readRDS(file = here::here("models/rf_model.rds"))

#### Step 2: Prepare Data for Predictions ####
# Year(s) for prediction
years <- 2025

# Create a data frame with unique states and years
prediction_data <- expand.grid(
  year = years,
  state = unique(clean_data$state)  # Use unique states
)

# Convert 'state' to a factor and match levels with training data
prediction_data$state <- factor(prediction_data$state, levels = levels(clean_data$state))

# Ensure no missing levels
if (any(is.na(prediction_data$state))) {
  stop("State levels in prediction_data do not match clean_data.")
}

# Make predictions
predictions <- predict(rf_model_clean, newdata = prediction_data)

# Add predictions to the data frame
prediction_data$predicted_casualties <- predictions

#### Step 3: Prepare Map Data ####
# Get US map data
us_map <- map_data("state")

# Convert state names in `us_map` to lowercase
us_map$region <- tolower(us_map$region)

# Convert state in `prediction_data` to lowercase for compatibility
prediction_data$state <- tolower(as.character(prediction_data$state))

# Merge map data with predictions
map_data <- us_map %>%
  left_join(prediction_data, by = c("region" = "state"))

# Fill missing predictions with 0 (optional, avoids grey states)
map_data$predicted_casualties[is.na(map_data$predicted_casualties)] <- 0

#### Step 4: Plot the Map ####
ggplot(map_data, aes(long, lat, group = group, fill = predicted_casualties)) +
  geom_polygon(color = "white") +
  scale_fill_gradient(
    low = "#E6F7FF",  # Light blue
    high = "#0072B2", # Dark blue
    name = "Casualties"
  ) +
  labs(
    title = "Predicted US School Shooting Casualties by State (2025)",
    subtitle = "Number of casualties visualized geographically",
    x = NULL,
    y = NULL
  ) +
  coord_fixed(1.3) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 10),
    plot.subtitle = element_text(hjust = 0.5, size = 10),
    legend.position = "right",
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  )

```


```{r}
#| echo: false
#| eval: false
#| label: tbl-modelresults
#| tbl-cap: "Explanatory models of flight time based on wing width and wing length"
#| warning: false

modelsummary::modelsummary(
  list(
    "First model" = first_model
  ),
  statistic = "mad",
  fmt = 2
)
```




# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this. 

## Second discussion point

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}


# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows... 

```{r}
#| eval: false
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

pp_check(first_model) +
  theme_classic() +
  theme(legend.position = "bottom")

posterior_vs_prior(first_model) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom") +
  coord_flip()
```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}
#| echo: false
#| eval: false
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

plot(first_model, "trace")

plot(first_model, "rhat")
```



\newpage


# References


